{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05477dbe",
   "metadata": {},
   "source": [
    "### Урок 4. Оценка и интерпретация полученной модели. Обсуждение курсового проекта."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5f360a",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00cd300",
   "metadata": {},
   "source": [
    "#### 1. Расскажите, как работает регуляризация в решающих деревьях, какие параметры мы штрафуем в данных алгоритмах?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f04bca-168b-4f60-9f5f-750327af9a00",
   "metadata": {},
   "source": [
    "Понятие \"регуляризация\" используется обычно для линейных моделей, где она отвечает за калибровку весовых коэффициентов. В лин. моделях регуляризация служит для предотвращения переобучения. В случае деревьев решений и их ансамблей можно говорить об аналогах регуляризации, а именно: уменьшении глубины деревьев и минимальном количестве значений целевой переменной в одном листе дерева. В случае обучения деревьев \"регуляризация\" даже более важна, чем при обучении лин.моделей, поскольку дерево, обучаясь неограниченно (понижая энтропию, Gini impurity или MSE), почти обязательно переобучится. \n",
    "Более полный список гиперпараметров, отвечающих за \"регуляризацию\" в деревьях решений scikit-learn:\n",
    "\n",
    "- min_samples_split : минимальное количество значений в узле для того, чтобы узел можно было разделить на ветви.\n",
    "- min samples leaf : минимальное количество значений, которое должно быть в листе\n",
    "- max leaf nodes : максимальное количество листьев\n",
    "- max features : максимальное количество признаков, которое участвует в определении необходимости разделения узла на ветви.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9100425c",
   "metadata": {},
   "source": [
    "#### 2. По какому принципу рассчитывается \"важность признака (feature_importance)\" в ансамблях деревьев?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8957189a-0b8d-4aac-b4a4-8be53cfe7c18",
   "metadata": {},
   "source": [
    "Feature importance - это методика присвоения определенного значения признаку в зависимости от его важности для построения дерева решений.\n",
    "Существует много способов оценки параметра feature importance в деревьях и в ансамблях деревьев. Дальнейшее описание относится к способу расчета feature importance в Decision tree и в Random forest библиотеки scikit-learn.\n",
    "\n",
    "Для вычисления feature importance для деревьев решений в задачах классификации минимизируется метрика Gini impurity, а для задач регрессии минимизируется MSE. Для вычисления Feature importance в ансамбле деревьев используется следующий алгоритм (источник: https://www.kaggle.com/kashnitsky/topic-5-ensembles-part-3-feature-importance):\n",
    "\n",
    "![Алгоритм](https://raw.githubusercontent.com/tyashin/GB-AI-Python-DS-libs-2-homeworks/main/lesson-4/lesson-4.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0b5096-6664-421f-b00f-2b9ed2fe16fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "language": "python",
   "name": "python385jvsc74a57bd08a49bb684e43ab00106799ded31d49934e1f306b7a86a273670123ff298874f2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
